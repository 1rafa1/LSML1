{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "debbed21-e72b-454a-b9b5-4b3c7ca75036",
   "metadata": {},
   "source": [
    "Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "235217a5-592c-43c5-b7d8-28906b57b101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import array_contains\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75136e83-10fb-4c82-8e66-fc9d6ddd5b04",
   "metadata": {},
   "source": [
    "Reading the file from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29c9659b-d5a3-4a77-a030-b5d3034095f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2023-10-04 20:09:05,895 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:\npy4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)\n\tat py4j.Gateway.invoke(Gateway.java:237)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest-mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39msetLogLevel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDEBUG\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/session.py:500\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    497\u001b[0m     sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m--> 500\u001b[0m     session \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(session\u001b[38;5;241m.\u001b[39m_jvm, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkSession$\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODULE$\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m     )\u001b[38;5;241m.\u001b[39mapplyModifiableSettings(session\u001b[38;5;241m.\u001b[39m_jsparkSession, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/session.py:589\u001b[0m, in \u001b[0;36mSparkSession.__init__\u001b[0;34m(self, sparkContext, jsparkSession, options)\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkSession$\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODULE$\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mapplyModifiableSettings(\n\u001b[1;32m    586\u001b[0m             jsparkSession, options\n\u001b[1;32m    587\u001b[0m         )\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 589\u001b[0m         jsparkSession \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparkSession\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkSession$\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODULE$\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mapplyModifiableSettings(\n\u001b[1;32m    592\u001b[0m         jsparkSession, options\n\u001b[1;32m    593\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/protocol.py:330\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:\npy4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)\n\tat py4j.Gateway.invoke(Gateway.java:237)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark Assignment\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e207419-574d-45f8-91c6-e654a8b69aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2023-10-04 20:07:59,766 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2023-10-04 20:08:02,386 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "rdd = sc.textFile('wrk/akhgabzhalilov/clickstream.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3965a53a-6229-4722-9067-0fabd0e88581",
   "metadata": {},
   "source": [
    "Removing header from the file and parsing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa7dcd6e-7ba0-4804-ac69-347069df6df1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rdd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rdd \u001b[38;5;241m=\u001b[39m \u001b[43mrdd\u001b[49m\u001b[38;5;241m.\u001b[39mmapPartitionsWithIndex(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m idx, it: islice(it, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m it \n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m rdd \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m l: l\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      5\u001b[0m rdd\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rdd' is not defined"
     ]
    }
   ],
   "source": [
    "rdd = rdd.mapPartitionsWithIndex(\n",
    "    lambda idx, it: islice(it, 1, None) if idx == 0 else it \n",
    ")\n",
    "rdd = rdd.map(lambda l: l.split('\\t'))\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacf6073-c516-4326-a12c-3bbd196b4c58",
   "metadata": {},
   "source": [
    "Schema for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b87762-e3d2-4242-be5e-517a9c437575",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = rdd.map(lambda p: Row(session_id=int(str(p[1]) + str(p[0])), event_type=str(p[2]), event_page=str(p[3]), timestamp=int(p[4])))\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cc49f9-0c9c-4736-90d6-adb0a879bfee",
   "metadata": {},
   "source": [
    "Creating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c289d49-6a51-48d0-b6db-1826d9396540",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(rdd)\n",
    "df.createOrReplaceTempView('clickstream')\n",
    "df.cache()\n",
    "df.show(5)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4725a459-a79d-4c5f-8538-4918b68ed827",
   "metadata": {},
   "source": [
    "Removing Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b7a0d0-a3e5-470c-b5d4-6f70b8ebb121",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['event_type'] != 'event']\n",
    "df.show(5)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39d5a29-0a08-4a4e-81a8-2ef4a2dbec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.orderBy(['session_id', 'timestamp'], ascending=True)\n",
    "df.show(15)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27c9a2d-a97c-47a7-ba89-aa56b6df13cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('session_id').count().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a327bf9-1d35-4937-a795-5ce3f44ffe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('session_id').count().orderBy('count', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32b78b5-c4e8-4510-9e30-70e533ace612",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['session_id'] == 1367635].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f88ea2-3928-4a4c-9edf-035dd11b87a3",
   "metadata": {},
   "source": [
    "Calculating the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3c99b7-b2d2-4b17-b980-68d9c222b645",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataCollect = df.collect()\n",
    "len(dataCollect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4b625c-d30f-4da0-8c6d-91d28e3d99f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9359bd62-ab6a-4531-aaf0-822de2a1cf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in dataCollect:\n",
    "    session_id = row['session_id']\n",
    "    event_type = row['event_type']\n",
    "    event_page = row['event_page']\n",
    "    \n",
    "    if session_id not in result:\n",
    "        result[session_id] = []\n",
    "        \n",
    "    if event_type == 'page':\n",
    "        result[session_id].append(event_page)\n",
    "    else:\n",
    "        result[session_id].append('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea0ebff-171e-4892-88a1-f5f549b068a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfed22f5-084e-4f07-afca-d237ef7cdb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf23937-0306-4852-883b-f1fa8652856d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in result.items():\n",
    "    # create route\n",
    "    route = []\n",
    "    for page in v:\n",
    "        if page == 'error': break\n",
    "        if len(route) > 0:\n",
    "            if route[-1] == page:\n",
    "                continue\n",
    "        route.append(page)\n",
    "        \n",
    "    route = '-'.join(route)\n",
    "        \n",
    "    # Add session_id, route to DataFrame\n",
    "    rows.append(Row(k, route))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d296a09b-ff1b-414e-9ac7-8545f612f491",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5a4d40-6e5a-4f5e-bd2d-7e638aba937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame(rows, ['session_id', 'route'])\n",
    "df2.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90884eb9-65d2-48fa-acb3-e4573fa04bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2['session_id'] == 507].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ba37c7-a3ce-49ed-acec-14360c67a43b",
   "metadata": {},
   "source": [
    "Proof-Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d92de52-1ca9-450b-9c94-1d6f27cc0cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['session_id'] == 514].show(50, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cdf07a-bd59-454e-a946-f5b17e137597",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.groupby('route').count().orderBy('count', ascending=False).limit(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfc7365-eba4-4e23-b7c6-056145a25af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938ba096-a729-46b1-a975-2340f78944f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df3.collect():\n",
    "    print(row)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4865cf-0828-4822-a45d-7558c552b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df3.limit(30).toPandas().to_string(index=False, header=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6b4efc-6cc8-4da6-ae82-1f93cdd8f7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bc63bf-038b-4384-bead-94ca3bef2bba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!hadoop fs -get sga_output.csv /home/akhgabzhalilov/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
